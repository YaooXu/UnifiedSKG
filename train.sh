export HF_HOME=/mnt/publiccache/huggingface
export HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1
export NCCL_P2P_DISABLE=1 NCCL_IB_DISABLE=1

deepspeed train.py --deepspeed deepspeed/ds_config_zero2.json --seed 2 --cfg Salesforce/T5_large_finetune_wikitq.cfg --run_name graph_T5_large_finetune_wikitq --logging_strategy steps --logging_first_step true --logging_steps 4 --evaluation_strategy steps --eval_steps 200 --metric_for_best_model avr --greater_is_better true --save_strategy steps --save_steps 200 --save_total_limit 1 --load_best_model_at_end --gradient_accumulation_steps 16 --num_train_epochs 100 --adafactor false --learning_rate 5e-5 --do_train --do_eval --do_predict --predict_with_generate --overwrite_output_dir --output_dir output/graph_T5_large_finetune_wikitq --per_device_train_batch_size 8 --per_device_eval_batch_size 16 --generation_num_beams 4 --generation_max_length 128 --input_max_length 1024 --ddp_find_unused_parameters true